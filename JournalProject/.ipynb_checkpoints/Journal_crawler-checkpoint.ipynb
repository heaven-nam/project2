{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22b56780",
   "metadata": {},
   "source": [
    "# Journal Crawler\n",
    "## 코드 설명\n",
    "1. df : 추후 CSV파일로 내보내기 위해 사용. 구성은 다음과 같음.\n",
    "    - 제목\n",
    "    - Abstract\n",
    "    - 저자\n",
    "    - 논문지\n",
    "    - 발행연도\n",
    "    - 해당 논문 링크\n",
    "    - 키워드\n",
    "2. runpages : 어떤 논문을 인용한 논문을 수집하기 위한 링크를 3페이지 정도 돌기위해 사용\n",
    "3. getAbstract : 논문의 링크에 들어가서 해당 웹페이지에서 abstract를 수집하기 위해 만들었으나, 저널별로 구조가 너무 달라 일단 보류중\n",
    "\n",
    "## 수집방법\n",
    "BFS를 사용한다. url pool에서 queue구조로 꺼내기위해 pop(0)를 사용.<br>\n",
    "parameter를 분리해서 검색어를 바꿔가면서 검색할 수 있다.<br>\n",
    "하나의 검색어가 들어가면 해당 결과페이지에서 4페이지까지 수집하며, \\[책\\]과 \\[PDF\\], \\[인용\\]은 제외한다.<br>\n",
    "결과 페이지에서 `div class=gs_ri`가 논문별로 정보를 가지고 있으며 이 정보에서 위 df를 채우기 위한 정보들을 가지고 온다.<br>\n",
    "위 정보 중 인용은 link이고 이 link를 가지고와서 3페이지 정도 검색하여 url pool에 추가한다. BFS를 위해 depth를 설정하였으며 depth는 3정도까지만 가지고 온다.<br>\n",
    "정보를 정제하여 dictionary로 만든다음 dataframe형태로 만들어 기존의 df dataframe에 concat한다.<br>\n",
    "수집이 완료되면 csv로 저장하여 db에 넣을 준비를 한다. $\\leftarrow$ 이 부분은 수정이 필요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f8a276",
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import urllib.parse \n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a8643b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['title','abstract','author','journal','year','pdf_link','keyword'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926cae82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runpages(url):\n",
    "    result = list()\n",
    "    pages = [0]\n",
    "    for i in pages:\n",
    "        newUrl = url + '&' + 'start='+ str(0+10*i)\n",
    "        result.append(newUrl)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2093c3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 얘는 망했어요\n",
    "\n",
    "def getAbstract(url):\n",
    "    web = get(url, headers=headers)\n",
    "    print(web.headers['Content-Type'])\n",
    "    dom = BeautifulSoup(web.text, 'html.parser')\n",
    "    metas = dom.find_all('meta')\n",
    "    for meta in metas:\n",
    "        if 'property' in meta.attrs.keys():\n",
    "            print(meta.attrs['property'])\n",
    "#         if meta.attrs['property'] != None:\n",
    "# #             print(meta)\n",
    "#     print(dom)\n",
    "#     cnt = 0\n",
    "#     for el in dom.find_all('div'):\n",
    "#         print(el.text.strip())\n",
    "#         if 'abstract' == el.text.lower():\n",
    "#             print(el)\n",
    "#             classInfo = el.attrs['class'][0]\n",
    "#     newel = dom.find('div',attrs={'class':classInfo}).find_parent()\n",
    "#     for el in newel:\n",
    "#         if 'abstract' not in el.text.lower():\n",
    "#             abstract = el.text\n",
    "#     return abstract\n",
    "url = 'https://ieeexplore.ieee.org/abstract/document/1456701'\n",
    "\n",
    "# getAbstract(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bf0e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e03c8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "baseurl = 'https://scholar.google.com/scholar'\n",
    "params = {\n",
    "    'hl':'ko',\n",
    "    'start':0,\n",
    "    'as_sdt':'0%2C5',\n",
    "    'q':'harmonic analysis',\n",
    "    }\n",
    "\n",
    "urls = list() # url담을 list\n",
    "visited = list()\n",
    "for i in [0,1]:\n",
    "    params['start'] = 0+10*i\n",
    "    url = baseurl + '?' + urllib.parse.urlencode(params)\n",
    "    urls.append((url,0))\n",
    "\n",
    "while urls:\n",
    "    seed = urls.pop(0)\n",
    "    print(seed[0])\n",
    "    visited.append(seed[0])\n",
    "    \n",
    "    if seed[1] > 2:\n",
    "        continue\n",
    "        \n",
    "    resp = get(seed[0], headers=headers)\n",
    "\n",
    "    if resp.status_code != 200:\n",
    "        print(resp.headers)\n",
    "        print(resp.status_code)\n",
    "        break\n",
    "        \n",
    "    else:\n",
    "        dom = BeautifulSoup(resp.text, 'html.parser')\n",
    "        \n",
    "        # 정보 뭉치 찾기\n",
    "        infos= dom.find_all('div', attrs={'class':'gs_ri'})\n",
    "\n",
    "        for info in infos:\n",
    "            isbook = info.find('span',attrs={'class':'gs_ctc'})\n",
    "            iscite = info.find('span',attrs={'class':'gs_ctu'})\n",
    "            if isbook != None and 'B' in isbook.text or iscite != None:\n",
    "                continue\n",
    "            else:\n",
    "                data = dict()\n",
    "                _ = info.find('h3',attrs={'class':'gs_rt'}).find('a')\n",
    "                citing = info.find('div',attrs={'class':'gs_fl'})\n",
    "                aTags = citing.find_all('a')\n",
    "                citeLink = 'https://scholar.google.com' + aTags[2].attrs['href']\n",
    "                citeLinks = runpages(citeLink)\n",
    "\n",
    "                for l in citeLinks:\n",
    "                    urls.append((l,seed[1]+1))\n",
    "                \n",
    "                link = _.attrs['href']\n",
    "                data['pdf_link'] = [link]\n",
    "                title = _.text\n",
    "                data['title'] = [title]\n",
    "\n",
    "                ajy = info.find('div',attrs={'class':'gs_a'}).text.split('-')\n",
    "\n",
    "                author = ajy[0].strip()\n",
    "                data['author'] = [author]\n",
    "                journal = ajy[1].strip()[:-4].split('\\xa0')[0]\n",
    "                data['journal'] = [journal]\n",
    "                year = ajy[1].strip()[-4:]\n",
    "                data['year'] = [year]\n",
    "                data['abstract'] = ['']\n",
    "                data = pd.DataFrame(data)\n",
    "                df = pd.concat([df,data])\n",
    "    print(len(urls),len(visited))\n",
    "    time.sleep(10)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0d204b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd9514a",
   "metadata": {},
   "source": [
    "df.to_csv('journal_1127.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd89a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1800a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = get('https://scholar.google.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7d7743",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81172062",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
